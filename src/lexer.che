use import string_database
use import error_handler

use import std.string
use import std.unicode
use import std.printable
use import std.io.fs
use import std.rc
use import std.ring_queue
use import std.array
use import std.thread

C  :: import std.c
io :: import std.io
mem :: import std.mem.allocator

#export_scope

LexerInputStream :: trait {
    get_name :: (&Self) -> string;
    get_text :: (&Self) -> Option[string];
}

StringInputStream :: struct {
    contents : string
    retrieved_contents := false
}

impl LexerInputStream for StringInputStream {
    get_name :: (&Self) -> string {
        return "string"
    }

    get_text :: (&Self) -> Option[string] {
        result : Option[string] = if self.retrieved_contents then None else Some(self.contents)
        @cast(&mut Self, self).retrieved_contents = true
        return result
    }
}

FileInputStream :: struct {
    filename : string
    contents : string
    retrieved_contents := false
}

impl LexerInputStream for FileInputStream {
    get_name :: (&Self) -> string {
        return self.filename
    }

    get_text :: (&Self) -> Option[string] {
        result : Option[string] = if self.retrieved_contents then None else Some(self.contents)
        @cast(&mut Self, self).retrieved_contents = true
        return result
    }
}

Lexer :: struct {
    input_stream: Rc[LexerInputStream]
    text        : String
    location    : Location
    last_token_location : Location
    next_queue  : RingQueue[Token]
    offset      : int
    string_db   : &StringDatabase
    indent      := 0
    new_indent  := -1
    do_stack    := Array[(line: int, indent: int)].new()

    // interned keywords
    KwLambda    : string
    KwReturn    : string
    Kwfn        : string
    KwFn        : string
    KwStruct    : string
    KwEnum      : string
    KwImpl      : string
    KwIf        : string
    KwElse      : string
    KwFor       : string
    KwWhile     : string
    KwLoop      : string
    KwAnd       : string
    KwOr        : string
    KwTrue      : string
    KwFalse     : string
    KwNull      : string
    KwUse       : string
    KwDefer     : string
    KwMatch     : string
    KwBreak     : string
    KwContinue  : string
    KwTrait     : string
    KwCast      : string
    KwConst     : string
    KwDefault   : string
    KwPub       : string
    KwThen      : string
    KwDo        : string
    KwMut       : string
    KwImport    : string
    KwIn        : string
    KwIs        : string
}

impl TextProvider for Lexer {
    get_text :: (&Self, filename: string) -> string {
        return text.slice()
    }
}

impl Lexer {
    // i dont know why, but for some reason when I enable trace-stack this function crashes
    // but with #nostacktrace it works...
    // - NO, 09.12.19
    from_stream :: (stream: Rc[LexerInputStream], string_db: &StringDatabase) -> Rc[Lexer] #nostacktrace {
        return Rc[Lexer].new(Lexer(
            input_stream    = stream.clone()
            text            = String.empty()
            location        = Location(
                file        = stream.get().get_name()
                byte_index  = 0
                byte_length = 1
                line        = 1
                column      = 1
            )
            last_token_location = Location(
                file        = stream.get().get_name()
                byte_index  = 0
                byte_length = 1
                line        = 1
                column      = 1
            )
            offset          = 0
            next_queue      = RingQueue[Token].new()
            string_db       = string_db

            KwLambda   = string_db.intern("lambda")
            KwReturn   = string_db.intern("return")
            Kwfn       = string_db.intern("fn")
            KwFn       = string_db.intern("fn")
            KwStruct   = string_db.intern("struct")
            KwEnum     = string_db.intern("enum")
            KwImpl     = string_db.intern("impl")
            KwIf       = string_db.intern("if")
            KwElse     = string_db.intern("else")
            KwFor      = string_db.intern("for")
            KwWhile    = string_db.intern("while")
            KwLoop     = string_db.intern("loop")
            KwAnd      = string_db.intern("and")
            KwOr       = string_db.intern("or")
            KwTrue     = string_db.intern("true")
            KwFalse    = string_db.intern("false")
            KwNull     = string_db.intern("null")
            KwUse      = string_db.intern("use")
            KwDefer    = string_db.intern("defer")
            KwMatch    = string_db.intern("match")
            KwBreak    = string_db.intern("break")
            KwContinue = string_db.intern("continue")
            KwTrait    = string_db.intern("trait")
            KwCast     = string_db.intern("cast")
            KwConst    = string_db.intern("const")
            KwDefault  = string_db.intern("default")
            KwPub      = string_db.intern("pub")
            KwThen     = string_db.intern("then")
            KwDo       = string_db.intern("do")
            KwMut      = string_db.intern("mut")
            KwImport   = string_db.intern("import")
            KwIn       = string_db.intern("in")
            KwIs       = string_db.intern("is")
        ))
    }

    from_string :: (content: string, string_db: &mut StringDatabase) -> Rc[Lexer] {
        return Lexer.from_stream(Rc[LexerInputStream].new(StringInputStream(content)), string_db)
    }

    from_file :: (filename: string, string_db: &mut StringDatabase) -> Result[Rc[Lexer], ()] {
        content := {
            content := try_with(read_file(filename), {return Err(())})
            string_db.intern(content.slice())
        }
        return Ok(Lexer.from_stream(Rc[LexerInputStream].new(FileInputStream(filename, content)), string_db))
    }

    current_location :: (&Self) -> Location {
        if next_queue.count() > 0 {
            return next_queue.peek().location
        } else {
            return self.location
        }
    }

    push_indent :: (&mut Self) {
        self.do_stack.push((self.location.line, self.indent))
    }

    expect_token :: (&mut Self, typ: TokenType) -> bool, Token {
        token := next_token()
        if int(token.typ) == int(typ) {
            return true, token
        } else {
            return false, token
        }
    }

    enqueue_token :: (&mut Self, token: Token) {
        self.last_token_location = token.location
        self.next_queue.push(token)
    }

    enqueue_eof :: (&mut Self) {
        if self.next_queue.count() > 0 and self.next_queue.peek_last().typ == .EOF {
            return
        }
        self.last_token_location = self.location
        self.next_queue.push(Token(
            typ      = .EOF
            data     = .None
            location = self.location
            suffix   = None
        ))
    }

    peek_token :: (&mut Self) -> &Token {
        if self.next_queue.count() == 0 {
            self.advance()
        }
        @assert(self.next_queue.count() > 0)
        return self.next_queue.peek()
    }

    next_token :: (&mut Self) -> Token {
        if self.next_queue.count() == 0 {
            self.advance()
        }
        @assert(self.next_queue.count() > 0)
        return self.next_queue.pop()
    }

    skip_line :: (&mut Self) {
        while true {
            tok := self.next_token()

            match tok.typ {
                TokenType.EOF -> {
                    return
                }
                TokenType.NewLine -> {
                    return
                }
            }
        }
    }

    advance :: (&mut Self) {
        skip_newlines_and_comments()

        // if self.do_stack.count() > 0 and self.do_stack.peek_last().line == location.line {
        //     self.do_stack.pop()
        // }

        mut token := Token(
            typ      = TokenType.EOF
            data     = TokenData.None
            location = location
            suffix   = None
        )

        token.location.byte_length = 0

        curr, curr_len := {
            x := try_with(peek_char(0), {
                self.enqueue_eof()
                return
            })
            x[0], int(x[1])
        }

        match curr {
            '<' -> {
                next, next_len := {
                    x := try_with(peek_char(1), (char(0), i32(0)))
                    x[0], int(x[1])
                }
                match next {
                    '-' -> simple_token(&token, TokenType.ReverseArrow,   curr_len + next_len, 2)
                    '=' -> simple_token(&token, TokenType.LessEqual,      curr_len + next_len, 2)
                    '<' -> simple_token(&token, TokenType.LessLess,       curr_len + next_len, 2)
                    _   -> simple_token(&token, TokenType.Less,           curr_len, 1)
                }
            }
            '>' -> {
                next, next_len := {
                    x := try_with(peek_char(1), (char(0), i32(0)))
                    x[0], int(x[1])
                }
                match next {
                    '=' -> simple_token(&token, TokenType.GreaterEqual,   curr_len + next_len, 2)
                    _   -> simple_token(&token, TokenType.Greater,        curr_len, 1)
                }
            }
            '=' -> {
                next, next_len := {
                    x := try_with(peek_char(1), (char(0), i32(0)))
                    x[0], int(x[1])
                }
                match next {
                    '>' -> simple_token(&token, TokenType.DoubleArrow,    curr_len + next_len, 2)
                    '=' -> simple_token(&token, TokenType.DoubleEqual,    curr_len + next_len, 2)
                    _   -> simple_token(&token, TokenType.Equal,          curr_len, 1)
                }
            }
            '!' -> {
                next, next_len := {
                    x := try_with(peek_char(1), (char(0), i32(0)))
                    x[0], int(x[1])
                }
                match next {
                    '=' -> simple_token(&token, TokenType.NotEqual,       curr_len + next_len, 2)
                    _   -> simple_token(&token, TokenType.Bang,           curr_len, 1)
                }
            }
            '+' -> {
                next, next_len := {
                    x := try_with(peek_char(1), (char(0), i32(0)))
                    x[0], int(x[1])
                }
                match next {
                    '=' -> simple_token(&token, TokenType.AddEq,          curr_len + next_len, 2)
                    _   -> simple_token(&token, TokenType.Plus,           curr_len, 1)
                }
            }
            '-' -> {
                next, next_len := {
                    x := try_with(peek_char(1), (char(0), i32(0)))
                    x[0], int(x[1])
                }
                match next {
                    '>' -> simple_token(&token, TokenType.Arrow,          curr_len + next_len, 2)
                    '=' -> simple_token(&token, TokenType.SubEq,          curr_len + next_len, 2)
                    _   -> simple_token(&token, TokenType.Minus,          curr_len, 1)
                }
            }
            '*' -> {
                next, next_len := {
                    x := try_with(peek_char(1), (char(0), i32(0)))
                    x[0], int(x[1])
                }
                match next {
                    '=' -> simple_token(&token, TokenType.MulEq,          curr_len + next_len, 2)
                    _   -> simple_token(&token, TokenType.Asterisk,       curr_len, 1)
                }
            }
            '/' -> {
                next, next_len := {
                    x := try_with(peek_char(1), (char(0), i32(0)))
                    x[0], int(x[1])
                }
                match next {
                    '=' -> simple_token(&token, TokenType.DivEq,            curr_len + next_len, 2)
                    _   -> simple_token(&token, TokenType.ForwardSlash,     curr_len, 1)
                }
            }
            '%' -> {
                next, next_len := {
                    x := try_with(peek_char(1), (char(0), i32(0)))
                    x[0], int(x[1])
                }
                match next {
                    '=' -> simple_token(&token, TokenType.ModEq,            curr_len + next_len, 2)
                    _   -> simple_token(&token, TokenType.Percent,          curr_len, 1)
                }
            }
            '.' -> {
                next, next_len := {
                    x := try_with(peek_char(1), (char(0), i32(0)))
                    x[0], int(x[1])
                }
                match next {
                    '.' -> {
                        next2, next_len2 := {
                            x := try_with(peek_char(2), (char(0), i32(0)))
                            x[0], int(x[1])
                        }
                        match next2 {
                            '=' -> simple_token(&token, TokenType.PeriodPeriodEq, curr_len + next_len + next_len2, 3)
                            _   -> simple_token(&token, TokenType.PeriodPeriod,   curr_len + next_len, 2)
                        }
                    }
                    _   -> simple_token(&token, TokenType.Period, curr_len, 1)
                }
            }
            ':' -> simple_token(&token, TokenType.Colon,            curr_len, 1)
            ';' -> simple_token(&token, TokenType.Semicolon,        curr_len, 1)
            '(' -> simple_token(&token, TokenType.OpenParen,        curr_len, 1)
            ')' -> simple_token(&token, TokenType.ClosingParen,     curr_len, 1)
            '{' -> simple_token(&token, TokenType.OpenBrace,        curr_len, 1)
            '}' -> simple_token(&token, TokenType.ClosingBrace,     curr_len, 1)
            '[' -> simple_token(&token, TokenType.OpenBracket,      curr_len, 1)
            ']' -> simple_token(&token, TokenType.ClosingBracket,   curr_len, 1)
            ',' -> simple_token(&token, TokenType.Comma,            curr_len, 1)
            '&' -> simple_token(&token, TokenType.Ampersand,        curr_len, 1)
            '^' -> simple_token(&token, TokenType.Hat,              curr_len, 1)
            '|' -> simple_token(&token, TokenType.Pipe,             curr_len, 1)

            '`n' -> {
                if do_stack.count() > 0 and do_stack.peek_last().line == self.location.line {
                    simple_token(&token, .OpenBrace, curr_len, 1)
                } else {
                    simple_token(&token, .NewLine, curr_len, 1)
                }
                self.new_indent = -1
                self.location.line += 1
                self.location.column = 1
            }

            '"' -> {
                parse_string_literal(&token, TokenType.StringLiteral, '"')
                parse_suffix(&token)
            }
            '`'' -> {
                parse_string_literal(&token, TokenType.CharLiteral, '`'')
                parse_suffix(&token)
            }

            // identifiers and keywords
            '$' -> {
                location.byte_index += curr_len
                location.column += 1
                parse_identifier(&token, TokenType.DollarIdentifier)
            }
            '#' ->  {
                location.byte_index += curr_len
                location.column += 1
                parse_identifier(&token, TokenType.HashIdentifier)
            }
            '@' ->  {
                location.byte_index += curr_len
                location.column += 1
                parse_identifier(&token, TokenType.AtSignIdentifier)
            }
            $x if is_ident_begin(x) -> {
                parse_identifier(&token, TokenType.Identifier)
                check_keywords(&token)
            }

            // number literal
            $x if is_digit(x) -> {
                parse_number_literal(&token)
                parse_suffix(&token)
            }

            _ -> {
                token.typ = TokenType.Unknown
                location.byte_index += curr_len
            }
        }

        token.location.byte_length = location.byte_index - token.location.byte_index
        token.location.end_column  = location.column
        token.location.end_line    = location.line

        self.enqueue_token(token)
    }

    parse_number_literal :: (&mut Self, token: &mut Token) {
        token.typ = TokenType.NumberLiteral
        mut base := 10
        mut str := {
            raw := @alloca(u8, 128)
            str := String.from_raw_ptr(raw.data, raw.length)
            str
        }

        mut is_float := false

        LexerNumberState :: enum #copy {
            Error
            Init
            Done
            Z
            X
            B
            DecDigit
            Dec_
            BinDigit
            Bin_
            HexDigit
            Hex_
            FloatPoint
            FloatDigit
            Float_
        }

        use LexerNumberState

        mut state := Init
        // while location.byte_index - offset < text.bytes.length {
        loop {
            c, c_len := try_with(peek_char(0), break)
            next, _  := try_with(peek_char(1), (char(0), i32(0)))

            match state {
                Error -> break
                Done  -> break

                Init  -> {
                    if c == '0' {
                        &str += c
                        state = Z
                    } else if is_digit(c) {
                        &str += c
                        state = DecDigit
                    }
                }

                Z -> match c {
                    'x' -> {
                        base = 16
                        str.resize(0)
                        state = X
                    }
                    'b' -> {
                        base = 2
                        str.resize(0)
                        state = B
                    }
                    '.' if next != '.' -> {
                        &str += c
                        state = FloatPoint
                    }
                    $x if is_digit(x) -> {
                        &str += x
                        state = DecDigit
                    }
                    '_' -> {
                        state = Dec_
                    }
                    _ -> {
                        state = Done
                    }
                }

                DecDigit -> match c {
                    '.' if next != '.' -> {
                        &str += c
                        state = FloatPoint
                    }
                    '_' -> {
                        state = Dec_
                    }
                    $x if is_digit(x) -> {
                        &str += c
                    }
                    _ -> {
                        state = Done
                    }
                }

                Dec_ -> match c {
                    $x if is_digit(x) -> {
                        &str += c
                        state = DecDigit
                    }
                    _ -> {
                        state = Error
                    }
                }

                FloatPoint -> {
                    is_float = true
                    if is_digit(c) {
                        &str += c
                        state = FloatDigit
                    } else {
                        state = Error
                    }
                }

                FloatDigit -> match c {
                    $c if is_digit(c) -> {
                        &str += c
                    }
                    '_' -> {
                        state = Float_
                    }
                    $_ -> {
                        state = Done
                    }
                }

                Float_ -> match c {
                    $x if is_digit(x) -> {
                        &str += c
                        state = FloatDigit
                    }
                    _ -> {
                        state = Error
                    }
                }

                X -> match c {
                    $c if is_hex_digit(c) -> {
                        &str += c
                        state = HexDigit
                    }
                    $_ -> {
                        state = Error
                    }
                }

                HexDigit -> match c {
                    $c if is_hex_digit(c) -> {
                        &str += c
                    }
                    '_' -> {
                        state = Hex_
                    }
                    $_ -> {
                        state = Done
                    }
                }

                Hex_ -> match c {
                    $x if is_hex_digit(x) -> {
                        &str += c
                        state = HexDigit
                    }
                    _ -> {
                        state = Error
                    }
                }

                B -> match c {
                    $c if is_binary_digit(c) -> {
                        &str += c
                        state = BinDigit
                    }
                    $_ -> {
                        state = Error
                    }
                }

                BinDigit -> match c {
                    $c if is_binary_digit(c) -> {
                        &str += c
                    }
                    '_' -> {
                        state = Bin_
                    }
                    $_ -> {
                        state = Done
                    }
                }

                Bin_ -> match c {
                    $x if is_binary_digit(x) -> {
                        &str += c
                        state = BinDigit
                    }
                    _ -> {
                        state = Error
                    }
                }
            }

            match state {
                Done -> break
                Error -> break
                $_   -> {
                    location.byte_index += int(c_len)
                    location.column += 1
                }
            }
        }

        match state {
            Error -> {
                token.typ = TokenType.Error
                token.data = TokenData.String("Invalid number literal")
                return
            }
        }

        &str += '`0'

        if is_float {
            d := C.strtod(cast str.get_raw(), null)
            token.data = TokenData.Double(d)
        } else {
            i := C.strtoll(cast str.get_raw(), null, cast base)
            token.data = TokenData.Integer(i)
        }
    }

    parse_suffix :: (&mut Self, token: &mut Token) {
        if is_ident_begin(try(peek_char(0))[0]) {
            start := location.byte_index - offset
            loop {
                c, c_len := try(peek_char(0))
                if !is_ident_char(c) then break
                location.byte_index += int(c_len)
                location.column += 1
            }

            token.suffix = Some(self.text.slice()[start .. location.byte_index - offset])
        }
    }

    simple_token :: (&mut Self, token: &mut Token, typ: TokenType, len: int, chars: int) {
        token.typ = typ
        location.byte_index += len
        location.column += chars
    }

    parse_identifier :: (&mut Self, token: &mut Token, typ: TokenType) {
        token.typ = typ
        start := location.byte_index - offset

        loop {
            c, c_len := try(peek_char(0))
            if !is_ident_char(c) then break
            location.byte_index += int(c_len)
            location.column += 1
        }

        str := self.text.slice()[start .. location.byte_index - offset]
        token.data = TokenData.String(string_db.intern(str))
    }

    parse_string_literal :: (&mut Self, token: &mut Token, typ: TokenType, end: char) {
        token.typ = typ
        location.byte_index += 1
        location.column += 1
        start := location.byte_index - offset

        mut foundEnd := false
        loop {
            c, c_len := try_with(peek_char(0), break)
            location.byte_index += int(c_len)
            location.column += 1

            if c == end {
                foundEnd = true
                break
            } else if c == '``' {
                try_with(peek_char(0), {
                    // TODO: report error
                    break
                })

                location.byte_index += int(c_len)
                location.column += 1
            }

            if c == '`n' {
                location.column = 1
                location.line += 1
            }
        }

        if !foundEnd {
            // TODO: report
        }

        str := self.text.slice()[start .. location.byte_index - offset - 1]
        token.data = TokenData.String(string_db.intern(str))
    }

    skip_newlines_and_comments :: (&mut Self) {
        loop {
            curr, curr_len := try_with(peek_char(0), break)

            match curr {
                '/' -> {
                    next, next_len := try_with(peek_char(1), (char(0), i32(0)))
                    match next {
                        '*' -> {
                            parse_multi_line_comment()
                            continue
                        }
                        '/' -> {
                            parse_single_line_comment()
                            continue
                        }

                        _ -> {}
                    }
                }

                ' ' -> {
                    self.location.byte_index += int(curr_len)
                    self.location.column += 1
                    continue
                }

                '`t' -> {
                    self.location.byte_index += int(curr_len)
                    self.location.column += 1
                    continue
                }

                '`n' -> {
                    break
                }
            }

            // non-whitespace character

            if do_stack.count() > 0 and do_stack.peek_last().line == self.location.line {
                // 'do' is followed by character which is not a newline, so don't do any indentation based block stuff
                @assert(self.new_indent == self.indent)
                do_stack.pop()
            } else if self.new_indent == -1 {
                // first character in this line
                // handle indentation based block ending

                self.new_indent = (self.location.column - 1) / 4

                if do_stack.count() > 0 and do_stack.peek_last().indent == self.indent and self.new_indent <= self.indent {
                    self.enqueue_token(Token(
                        typ      = .Error
                        location = self.location
                        data     = .String("Missing indentation")
                    ))
                    self.enqueue_token(Token(
                        typ      = .OpenBrace
                        location = self.location
                    ))
                    self.enqueue_token(Token(
                        typ      = .ClosingBrace
                        location = self.location
                    ))
                    do_stack.pop()
                } else {
                    while self.indent > self.new_indent {
                        self.indent -= 1
                        if do_stack.count() > 0 and do_stack.peek_last().indent == self.indent {
                            self.enqueue_token(Token(
                                typ      = .ClosingBrace
                                location = self.last_token_location.end()
                            ))
                            self.enqueue_token(Token(
                                typ      = .NewLine
                                location = self.last_token_location.end()
                            ))
                            do_stack.pop()
                        }
                    }
                }

                self.indent = self.new_indent
            }
            break
        }
    }

    peek_char :: (&Self, mut offset: int) -> Option[(char, i32)] {
        mut index := self.location.byte_index - self.offset
        while offset > 0, offset -= 1 {
            if index >= self.text.get_length() {
                match self.input_stream.get().get_text() {
                    Some($t) -> self.text.append_string(t)
                    None -> {
                        return None
                    }
                }
            }
            _, len := Utf8.decode(self.text.slice().bytes[index..])
            index += int(len)
        }

        if index >= self.text.get_length() {
            match self.input_stream.get().get_text() {
                Some($t) -> self.text.append_string(t)
                None -> {
                    return None
                }
            }
        }
        if index >= self.text.get_length() {
            return None
        }

        return Some(Utf8.decode(self.text.slice().bytes[index..]))
    }

    parse_multi_line_comment :: (&mut Self) {
        mut level := 0
        loop {
            curr, curr_len := try_with(peek_char(0), break)
            next, next_len := try_with(peek_char(1), (char(0), i32(0)))

            if curr == '/' and next == '*' {
                self.location.byte_index += int(next_len)
                self.location.column += 1
                level += 1
            } else if curr == '*' and next == '/' {
                self.location.byte_index += int(next_len)
                self.location.column += 1
                level -= 1

                if level == 0 {
                    break
                }
            } else if curr == '\n' {
                self.location.line += 1
                self.location.column = 1
            }

            self.location.byte_index += int(curr_len)
            self.location.column += 1
        }
    }

    parse_single_line_comment :: (&mut Self) {
        loop {
            next, len := try_with(peek_char(0), break)
            if next == '`n' {
                break
            }

            self.location.byte_index += int(len)
            self.location.column += 1
        }
    }

    check_keywords :: (&Self, token: &mut Token) {
        match token.data {
            TokenData.String($str) -> {
                token.typ =
                    if string.same(str, KwLambda)          then TokenType.KwLambda
                    else if string.same(str, KwReturn)     then TokenType.KwReturn
                    else if string.same(str, Kwfn)         then TokenType.Kwfn
                    else if string.same(str, KwFn)         then TokenType.KwFn
                    else if string.same(str, KwStruct)     then TokenType.KwStruct
                    else if string.same(str, KwEnum)       then TokenType.KwEnum
                    else if string.same(str, KwImpl)       then TokenType.KwImpl
                    else if string.same(str, KwIf)         then TokenType.KwIf
                    else if string.same(str, KwElse)       then TokenType.KwElse
                    else if string.same(str, KwFor)        then TokenType.KwFor
                    else if string.same(str, KwWhile)      then TokenType.KwWhile
                    else if string.same(str, KwLoop)       then TokenType.KwLoop
                    else if string.same(str, KwAnd)        then TokenType.KwAnd
                    else if string.same(str, KwOr)         then TokenType.KwOr
                    else if string.same(str, KwTrue)       then TokenType.KwTrue
                    else if string.same(str, KwFalse)      then TokenType.KwFalse
                    else if string.same(str, KwNull)       then TokenType.KwNull
                    else if string.same(str, KwUse)        then TokenType.KwUse
                    else if string.same(str, KwDefer)      then TokenType.KwDefer
                    else if string.same(str, KwMatch)      then TokenType.KwMatch
                    else if string.same(str, KwBreak)      then TokenType.KwBreak
                    else if string.same(str, KwContinue)   then TokenType.KwContinue
                    else if string.same(str, KwTrait)      then TokenType.KwTrait
                    else if string.same(str, KwCast)       then TokenType.KwCast
                    else if string.same(str, KwConst)      then TokenType.KwConst
                    else if string.same(str, KwDefault)    then TokenType.KwDefault
                    else if string.same(str, KwPub)        then TokenType.KwPub
                    else if string.same(str, KwThen)       then TokenType.KwThen
                    else if string.same(str, KwMut)        then TokenType.KwMut
                    else if string.same(str, KwImport)     then TokenType.KwImport
                    else if string.same(str, KwIn)         then TokenType.KwIn
                    else if string.same(str, KwIs)         then TokenType.KwIs
                    else if string.same(str, KwDo)         then TokenType.KwDo
                    else token.typ
            }
        }
    }
}

// token
Location :: struct #copy {
    file        : string = default
    byte_index  : int    = default
    byte_length : int    = default
    line        : int    = default
    column      : int    = default
    end_column  : int    = default
    end_line    : int    = default
}

impl Location {
    to :: (Self, end: Location) -> Location {
        return Location(
            file        = file
            byte_index  = byte_index
            byte_length = end.byte_index + end.byte_length - byte_index
            line        = line
            column      = column
            end_column  = end.end_column
            end_line    = end.end_line
        )
    }

    beginning :: (Self) -> Location {
        return Location(
            file        = file
            byte_index  = byte_index
            byte_length = 0
            line        = line
            column      = column
            end_column  = column
            end_line    = line
        )
    }

    end :: (Self) -> Location {
        return Location(
            file        = file
            byte_index  = byte_index + byte_length
            byte_length = 0
            line        = end_line
            column      = end_column
            end_column  = end_column
            end_line    = end_line
        )
    }

    with_line :: (Self, line: int) -> Location {
        mut result := self
        result.line = line
        return result
    }
}

Token :: struct #copy {
    typ         : TokenType
    location    : Location
    suffix      := Option[string].None
    data        := TokenData.None
}

TokenData :: enum #copy {
    None
    String  : string
    Integer : int
    Double  : double
}

impl TokenData {
    get_string :: (&Self) -> string {
        return match *self {
            .String($str) -> str
            _ -> @assert(false, "Not a string")
        }
    }
}

impl Printable for Location {
    print :: (&Self, str: &mut String, format: string) {
        str.appendf("{}:{}:{}", (file, line, column, end_line))
    }
}

impl Printable for Token {
    print :: (&Self, str: &mut String, format: string) {
        str.appendf("{} ({})", (typ, location))
        match data {
            TokenData.String($s) -> str.appendf(" String({})", s)
            TokenData.Integer($s) -> str.appendf(" Int({})", s)
            TokenData.Double($s) -> str.appendf(" Double({})", s)
        }

        match suffix {
            Some($s) -> str.appendf(" Suffix(`"{}`")", (s))
        }
    }
}

impl Printable for TokenType {
    print :: (&Self, str: &mut String, format: string) {
        use TokenType

        str += cast(string)match self {
            Error             -> "Error"
            Unknown           -> "Unknown"
            NewLine           -> "NewLine"
            EOF               -> "EOF"
            StringLiteral     -> "StringLiteral"
            CharLiteral       -> "CharLiteral"
            NumberLiteral     -> "NumberLiteral"
            Identifier        -> "Identifier"
            DollarIdentifier  -> "DollarIdentifier"
            HashIdentifier    -> "HashIdentifier"
            AtSignIdentifier  -> "AtSignIdentifier"
            ReplaceIdentifier -> "ReplaceIdentifier"
            Semicolon         -> "Semicolon"
            Colon             -> "Colon"
            Comma             -> "Comma"
            Period            -> "Period"
            PeriodPeriod      -> "PeriodPeriod"
            PeriodPeriodEq    -> "PeriodPeriodEq"
            Equal             -> "Equal"
            Ampersand         -> "Ampersand"
            Hat               -> "Hat"
            Bang              -> "Bang"
            Plus              -> "Plus"
            Minus             -> "Minus"
            Asterisk          -> "Asterisk"
            ForwardSlash      -> "ForwardSlash"
            Percent           -> "Percent"
            AddEq             -> "AddEq"
            SubEq             -> "SubEq"
            MulEq             -> "MulEq"
            DivEq             -> "DivEq"
            ModEq             -> "ModEq"
            Less              -> "Less"
            LessEqual         -> "LessEqual"
            Greater           -> "Greater"
            GreaterEqual      -> "GreaterEqual"
            DoubleEqual       -> "DoubleEqual"
            NotEqual          -> "NotEqual"
            ReverseArrow      -> "ReverseArrow"
            Arrow             -> "Arrow"
            DoubleArrow       -> "DoubleArrow"
            LessLess          -> "LessLess"
            OpenParen         -> "OpenParen"
            ClosingParen      -> "ClosingParen"
            OpenBrace         -> "OpenBrace"
            ClosingBrace      -> "ClosingBrace"
            OpenBracket       -> "OpenBracket"
            ClosingBracket    -> "ClosingBracket"
            Pipe              -> "Pipe"
            KwLambda          -> "KwLambda"
            KwReturn          -> "KwReturn"
            Kwfn              -> "Kwfn"
            KwFn              -> "KwFn"
            KwStruct          -> "KwStruct"
            KwEnum            -> "KwEnum"
            KwImpl            -> "KwImpl"
            KwIf              -> "KwIf"
            KwElse            -> "KwElse"
            KwFor             -> "KwFor"
            KwWhile           -> "KwWhile"
            KwLoop            -> "KwLoop"
            KwAnd             -> "KwAnd"
            KwOr              -> "KwOr"
            KwTrue            -> "KwTrue"
            KwFalse           -> "KwFalse"
            KwNull            -> "KwNull"
            KwUse             -> "KwUse"
            KwDefer           -> "KwDefer"
            KwMatch           -> "KwMatch"
            KwBreak           -> "KwBreak"
            KwContinue        -> "KwContinue"
            KwTrait           -> "KwTrait"
            KwCast            -> "KwCast"
            KwConst           -> "KwConst"
            KwDefault         -> "KwDefault"
            KwPub             -> "KwPub"
            KwThen            -> "KwThen"
            KwDo              -> "KwDo"
            KwMut             -> "KwMut"
            KwImport          -> "KwImport"
            KwIn              -> "KwIn"
            KwIs              -> "KwIs"
        }
    }
}

TokenType :: enum #copy {
    Error
    Unknown
    NewLine
    EOF
    StringLiteral
    CharLiteral
    NumberLiteral
    Identifier
    DollarIdentifier
    HashIdentifier
    AtSignIdentifier
    ReplaceIdentifier
    Semicolon
    Colon
    Comma
    Period
    PeriodPeriod
    PeriodPeriodEq
    Equal
    Ampersand
    Hat
    Bang
    Plus
    Minus
    Asterisk
    ForwardSlash
    Percent
    AddEq
    SubEq
    MulEq
    DivEq
    ModEq
    Less
    LessEqual
    Greater
    GreaterEqual
    DoubleEqual
    NotEqual
    ReverseArrow
    Arrow
    DoubleArrow
    LessLess
    OpenParen
    ClosingParen
    OpenBrace
    ClosingBrace
    OpenBracket
    ClosingBracket
    Pipe
    KwLambda
    KwReturn
    Kwfn
    KwFn
    KwStruct
    KwEnum
    KwImpl
    KwIf
    KwElse
    KwFor
    KwWhile
    KwLoop
    KwAnd
    KwOr
    KwTrue
    KwFalse
    KwNull
    KwUse
    KwDefer
    KwMatch
    KwBreak
    KwContinue
    KwTrait
    KwCast
    KwConst
    KwDefault
    KwPub
    KwThen
    KwDo
    KwMut
    KwImport
    KwIn
    KwIs
}

#file_scope
is_ident_begin :: (c: char) -> bool {
    return (c >= 'a' and c <= 'z') or (c >= 'A' and c <= 'Z') or (c == '_') or u32(c) > 127
}

is_ident_char :: (c: char) -> bool {
    return is_ident_begin(c) or (c >= '0' and c <= '9')
}

is_digit :: (c: char) -> bool {
    return c >= '0' and c <= '9'
}

is_hex_digit :: (c: char) -> bool {
    return (c >= '0' and c <= '9') or (c >= 'a' and c <= 'f') or (c >= 'A' and c <= 'F')
}

is_binary_digit :: (c: char) -> bool {
    return c >= '0' and c <= '1'
}